- The article is based on the original text by Caixian Chen, published on July 28, 2023, and centers around developments in the field of artificial intelligence (AI).
- The piece starts with Zhiyuan Liu's discovery of a groundbreaking AI paper on October 11, 2018, which gained traction among Chinese AI researchers.
- The paper in question - "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù - has been cited over 70,000 times in Google Scholar.
- The emphasis is on the term "paradigm" and its repeated occurrence to describe pivotal shifts in AI: once for deep learning in 2012, once for BERT in 2018, and once for advancements in Chinese large AI models before the release of ChatGPT in 2022.
- AI has three central disciplines: computer vision (CV), natural language processing (NLP), and machine learning (ML). BERT revolutionized the area of NLP, proposing pre-training of large amounts of labeled data to enhance algorithm performance.
- The innovation BERT brought is a two-way training methodology, allowing algorithms to perform multiple language tasks at the same time, and soon after its release, BERT clinched the top position on the GLUE language understanding list.
- Researchers from Tsinghua University, Beijing, led by Zhiyuan Liu and Maosong Sun, and Huawei's Noah's Ark Laboratory developed their pre-trained language model, ERNIE, and submitted it for the ACL top NLP academic conference in 2019.
- Baidu, another giant in the field, unveiled their pre-trained language model with the same name - ERNIE - following the Baidu's model refinement, they named this advanced iteration "Wen Xin" in Chinese and reserved "ERNIE" for the English name.
- The field's focus shifted when OpenAI released GPT-3 in June 2020, a pre-trained language model with a staggering 175 billion parameters - this was 500 times more than BERT and exceeded BERT in multiple language understanding tasks.
- OpenAI claims to have used 10,000 graphics cards for GPT-3's training, amounting to approximately 12 million US dollars, more than 80 million RMB.
- The impact of GPT-3 prompted Chinese researchers to focus more on studying large-scale pre-trained language models, or "big models," despite the hefty computational power requirement.
- Zhiyuan Liu collaborated with the Beijing Academy of Artificial Intelligence (BAAI), an institution less than two years old at the time, for research into large models.
- After several decisive discussions, BAAI managed to attract AI scholars through various initiatives and eventually formed a collective research team focused on large model studies. 
- BAAI made the call to heroes via multiple channels for scholars interested in collaborating on large models.- The Beijing Academy of Artificial Intelligence (BAAI) launched a large-scale model project, which drew interest from academics from Tsinghua University, Renmin University of China, Peking University and the Chinese Academy of Sciences, as well as professionals outside of the BAAI such as Hongxia Yang from the Alibaba DAMO Academy. The project involved approximately 100 individuals with Jie Tang appointed as the general director.
- The project was enthusiastically supported by Chen Jining, the mayor of Beijing during the time, and the city provided special funds for purchasing computing power.
- In October 2020, the consensus was reached to promote the R&D of Chinese large-scale models, focusing on four principal areas: Chinese-language large-scale models, multi-modal large-scale models, cognitive large-scale models, and protein large-scale models.
- The project team was split into four groups and collectively named their four imperial libraries after "Siku Quanshu" in the Qing Dynasty: "Wenyuan", "Wenlan", "Wenhui", and "Wensu". The project was also codenamed "Enlightenment" (wudao).
- Although other countries already had large language models like GPT-3, China lacked a large-scale pre-trained language model with a parameter volume of 175 billion or more, particularly in Chinese. This led to the decision to develop domestic large scale models.
- In October 2020, BAAI began to invest heavily in computing power, planning to buy 1000P computing power with the help of special city funds.
- Despite some criticism about the cost and lack of original innovation in developing large models, the team persisted, discovering the challenges and complexities involved.
- In December 2020, the Wenyuan team released the world's first Chinese open-source model "CPM" with 2.6 billion parameters.
- The Wenlan's BriVL 1.0 multimodal model based on 30 million image-text training data pairs and Wenhui's 10 billion parameter GLM-10B Chinese-English bilingual large model were released and combined to "Wudao 1.0" in March 2021, introducing China to large models.
- Wudao 2.0 was born in June 2021 after running multiple large models on supercomputer "Sunway OceanLight" for ten days, resulting in several trillion-parameter models. The most efficient model had 1.75 trillion parameters, ten times larger than GPT-3.
- "Wudao 2.0" includes two tens of billions models of the Wenyuan team, a 198 billion parameter Chinese-English bilingual MoE model (CPM 2.0), and the Wenlan team's 5 billion parameter graphic retrieval large model BriVL 2.0.
- Despite the challenges and risks involved, the successful release of "Wudao 2.0" positioned BAAI's Wudao as the flagbearer of Chinese large-scale models and its members became the pioneers in China's large-scale models.