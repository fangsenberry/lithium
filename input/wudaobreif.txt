
A Brief History of Large Models in Wudaokou
Note: These are Jeffrey Ding’s informal and unofficial translations -- all credit for the original goes to the authors and the original text linked below. These are informal translations and all credit for the original work goes to the authors. Others are welcome to share excerpts from these translations as long as my original translation is cited. Commenters should be aware that the Google Doc is also publicly shareable by link. These translations are part of the ChinAI newsletter - weekly-updated library of translations from Chinese thinkers on AI-related issues: https://chinai.substack.com/ 
________________________________________________________________________________
Source: Leiphone 雷峰网
Author: 陈彩娴 (Caixian Chen)

Date: July 28, 2023
Original Mandarin: https://mp.weixin.qq.com/s/fm37ofUwLQyItKkkLMjG5Q

"The best way to foresee the future is to create it yourself."

01 "Paradigm" Revolution
The beginning of the story happened in the autumn of 2018, in Haidian District, Beijing.
On that day, October 11, an ordinary Thursday, Zhiyuan Liu opened up the arXiv page as usual to browse the latest work in the field of artificial intelligence (AI) uploaded by scholars from all over the world. Most of the time, the quality of papers on arXiv is uneven, and Zhiyuan Liu only browsed roughly to get general information; but this day, he was profoundly drawn to a paper attached to the Google language team.

Originally, I just clicked in and glanced at it, but the more I looked at it, the more fascinated and surprised I became. After turning off the computer, I still couldn’t regain my senses for a long time, and was overwhelmed by the thoughts in it. Sure enough, he soon discovered that this paper also attracted widespread attention from other AI scholars in China. Professors and students from universities such as Tsinghua University, Peking University, Renmin University of China, and Fudan University were also enthusiastically discussing the work.

Everyone had a vague feeling: "This may be another technological paradigm revolution in the field of AI.”

This work is the famous BERT paper that has been cited more than 70,000 times in Google Scholar - "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.”



Paper link: https://arxiv.org/pdf/1810.04805.pdf

In the Chinese context, "paradigm" [范式] is not a common word. However, during Leiphones interviews about large models, this word was repeatedly mentioned, once to describe the deep learning in 2012, another time for BERT in 2018, and the other was the pioneering direction of Chinese large models before ChatGPT came out in 2022: "At that time, everyone did not think in the direction of general artificial intelligence (AGI), but felt that the large model could be made into a general artificial intelligence paradigm." This is part of a story that is to come. 

Back to BERT.

The word "范式" comes from the English word "paradigm", which means the basic system and structure of a field. For example, suits and Hanfu are two different paradigms in the field of clothing. On the basis of these two paradigms, fashion designers can design a variety of different styles of clothing. In short, a paradigm represents a change in underlying thinking, dividing the past from the future.

BERT's "two-way pre-training" idea embodies this potential.

AI has three major directions: computer vision (CV), natural language processing (NLP) and machine learning (ML). The ultimate goal of NLP is to allow computers to understand human language. So, how do you tell if a computer has understood human language? For a long time before BERT, the research idea of NLP was to split language understanding into small task directions, such as machine translation, text comparison, semantic analysis, etc., and then design, implement, and train AI algorithms for each task. For example, Zhiyuan Liu's research direction during his PhD (2006-2011) was a basic task of NLP, called "Keyword Extraction".

The difference between BERT and traditional methods is that traditional statistical learning or deep learning allows AI algorithms to directly learn the data of a certain task (such as text comparison). Before learning from this data, AI is like a blank sheet of paper without any foundational abilities. The trained algorithm can only perform one task; however, BERT's pre-training method is to let AI recite a large amount of labeled data before learning the task data, which is equivalent to going through test papers before the exam, so the trained algorithm performed even better in the subsequent actual combat of the "examination.”

BERT is not the first language model to adopt a pre-training method; GPT-1 released by OpenAI, a few months ago, was also a pre-trained language model. But the innovation of BERT is that it uses the idea of two-way training to break the dependence of the original pre-training method on the specified task framework.

GPT-1 is a one-way structure and can only learn text information from left to right or from right to left, so the trained algorithm can only perform one language task. For example, GPT-1 is good at text generation, but not good at understanding; and BERT is a two-way structure, which can learn language representation from the left and right at the same time, and learn on a large amount of unlabeled data for multiple tasks, so it can perform multiple language tasks such as knowledge question answering, context filling, and text understanding at the same time. And its performance on various tasks surpassed all the models at that time, and soon ranked first on the authoritative list of language understanding GLUE.

Everyone is shocked by the effect of BERT, just like back in 2012 when deep learning first showed its power:
***Excised two paragraphs revisiting the history of deep learning

Zhiyuan Liu belongs to Tsinghua University Natural Language Processing Lab (THUNLP), and Maosong Sun is the director of the laboratory. In 2012, Maosong Sun took the lead in applying for a 973 project of the Ministry of Science and Technology. In order to better understand the future technical route of NLP, he organized several units including Peking University, Harbin Institute of Technology, University of Science and Technology of China, and Baidu to discuss together. Everyone was unanimously optimistic about deep learning, so after the successful application of the project, THUNLP also turned to deep learning in 2013. Later, deep learning really swept the world.

Since then, "Dare to revolutionize yourself" has become the research spirit of THUNLP. After BERT came out, Zhiyuan Liu also quickly decided to turn to the pre-training method. Their idea is to use knowledge map methods to extract abstract knowledge, and then inject it into the pre-trained language model to make the model more intelligent. They cooperated with Qun Liu and Xin Jiang of Huawei's Noah's Ark Laboratory, and quickly developed a pre-trained language model, named it "ERNIE", and submitted it to the top NLP academic conference ACL in 2019.


Maosong Sun (left), Zhiyuan Liu (right)

Coincidentally, in 2018, Baidu's NLP team was also shocked by BERT, and almost simultaneously completed a pre-trained language model, which was first published on arXiv and also named "ERNIE". Both teams are named after the characters of the American cartoon "Sesame Street", because the previous pre-trained models such as ELMO and BERT are all characters in "Sesame Street". Google used BERT, and their goal was to benchmark against Google, so they thought of going together.

Both ERNIEs outperformed BERT on some tasks. Baidu published on arXiv first, and the papers of THUNLP and Huawei were accepted later. In order to distinguish it from Baidu, Zhiyuan Liu and others changed the name of the model, and so Baidu has continued to use the ERNIE name. Later, Baidu refined the large model, which was called "Wen Xin" in Chinese, and "ERNIE" in English.

Unsurprisingly, pre-training quickly became a mainstream approach in the NLP field. During the same period, some international teams also had a keen sense of smell and quickly caught up with BERT's two-way pre-training method. In February 2019, OpenAI released GPT-2. Although GPT-2 is better than GPT-1, it is still inferior to BERT in many language tasks, so OpenAI’s voice was completely suppressed by Google at that time.

But a year and a half later, history was refreshed again:

In June 2020, OpenAI suddenly released a research result beyond everyone's imagination - GPT-3 with a parameter scale of up to 175 billion. It is also a pre-trained language model, but GPT-3 has 500 times more parameters than BERT. It could not only generate language, but also surpassed BERT in various language understanding tasks.

Everyone's research worldview was turned upside down.

02 GPT-3 Lights a Fire
No one thought that the so-called "intelligent emergence" would appear after enlarging the number of the parameters for pre-trained language models. Google's supporting paper on the phenomenon was only published a year later.

The number of parameters of BERT is 340 million, which deserved the "big model" label compared with all language models in 2018, but everyone pays more attention to its pre-training method, and never thought of it as directly as OpenAI in terms of “accumulation volume.” GPT-3’s accumulation behaves as if the AI model had directly memorized the entire library.

As a result, the rote memorized GPT-3 not only became very strong in understanding, but also had a certain reasoning ability. Even on some unlabeled data and tasks, GPT-3 can apply newly acquired skills and knowledge immediately, and achieve good results.
***Excised two paragraphs about GPT-3 and the Turing Test

Even more outrageous, OpenAI claims to have used 10,000 graphics cards when training GPT-3.

Generally speaking, in scientific research in colleges and universities, the cost of computing power equipment only accounts for about 20% of a professor's overall scientific research funding, and those with more than 500 cards are rich players in the academic world. Previously, AI scientists at home and abroad mostly used a single card or multiple cards in a single machine when studying NLP, but GPT-3 training used a total of 10,000 cards, which converted to about 12 million US dollars, more than 80 million RMB.

From the perspective of engineering construction, the engineering difficulty of training GPT-3 is also unprecedented. Taking BERT as an example, the BERT training engineering volume of 340 million parameters is compared with the GPT-3 training engineering volume of 175 billion parameters, just like the difference between building a toy car and building an airplane. The engineering volume of a toy car is not suitable for aviation aircraft, and similarly, the training knowledge of small language models in the past is not suitable for large models.
GPT-3's crushing of BERT is essentially the crushing of "large-scale pre-trained language model" on "pre-trained language model".

On the one hand, everyone is excited about GPT-3; on the other hand, they feel a huge gap in their hearts. Before this, most Chinese scholars felt good about themselves, and felt that the level of papers published by domestic teams was comparable to that of American universities; after GPT-3 came out, they realized that there was still such a big gap between themselves and the top international level.

In Wudaokou, Beijing in the summer of 2020, computer science and AI scholars from Tsinghua University, Peking University, Renmin University, Chinese Academy of Sciences and other universities are paying attention to GPT-3. Although no one could explain the powerful mechanism of GPT-3 at the time, the intuition told everyone that this was an important watershed in the field of AI. The impact of GPT-3 was so great that some scholars decided that they would study large-scale pre-trained language models, or "big models" for short, no matter what.

Zhiyuan Liu is one of them. At that time, computing power was the most prominent obstacle in the study of large models. Zhiyuan Liu went to Chen Wenguang, Han Wentao and other teachers in the high-performance computing department of Tsinghua University to cooperate on research, and wanted to use distributed accelerated computing to reduce the training cost of large models. At the same time, he was also looking beyond THUNLP to seek outside help.

At that time, Maosong Sun served as the chief scientist in the direction of natural language processing in a new artificial intelligence research and development institution less than 100 meters away from the east gate of Tsinghua University, where Zhiyuan Liu also served as a young scientist. Naturally, Zhiyuan Liu thought of going to this organization to discuss cooperation.

This institution is now the famous Beijing Academy of Artificial Intelligence (BAAI).



But at that time, BAAI was only a research unit under development and construction, which had been established for only one and a half years.

The establishment of BAAI is a corner(stone) for the construction blueprint of the Beijing International Innovation Center. It was established under the joint guidance of the Ministry of Science and Technology and Beijing Municipality, and shoulders the mission of exploring the frontiers of artificial intelligence. Through projects such as "Zhiyuan Scholars Plan", "Zhiyuan Conference" and "QingYuan Club", BAAI has connected about 100 outstanding AI scientists in the Beijing area. Scholars are working together to find the "next big thing" in the field of AI.

Huang Tiejun, director of BAAI, told Leiphone that the selection of Zhiyuan scholars is very strict, so after selecting the corresponding scholars, BAAI will provide corresponding financial support to Zhiyuan scholars and does not require submission of research results; on the contrary, BAAI is more concerned about everyone being able to explore a major AI direction worth investing in.

In April 2019, BAAI established several major directions, including natural language processing, machine learning, information retrieval, etc., and 5 to 10 well-known scholars gathered for discussion in each direction. There are Maosong Sun, He Xiaodong, Zhiyuan Liu, etc. in the direction of natural language processing, and Jirong Wen, Jie Tang, etc. in the direction of intelligent information retrieval. After GPT-3 came out, scholars in several major directions are discussing GPT-3 and discussing how to research China's large models.

Before finally reaching a consensus, BAAI went through several rounds of important discussions.

The first two times were at Yanqi Lake in Beijing: at a meeting on machine learning in July 2020, Zhiyuan scholars believed that GPT-3 is a large phenomenon, and now that large language models have come out, we should research large vision models. However, after the discussion, they felt that large visual models consumed more computing power, so they did not put this plan into action. August had a meeting on the topic of information retrieval and mining. Jirong Wen, Jie Tang, and others discussed large-scale language models at the meeting. By September, at BAAI’s office meeting, Zhiyuan Liu proposed to study general language models.

After the National Day, on October 10th, BAAI held another discussion in Yanqi Lake, inviting scholars from different fields to participate, and finally reached a consensus at the meeting to form a research team to cooperate in the direction of large models.

After the establishment of the project, BAAI issued "Hero Posts" on various channels, inviting scholars who are interested in large models to study together, and said that "Heroes don't ask where they come from." As soon as the convening order came out, it coincided with the ideas of all the scholars, and everyone signed up one after another.

The first to raise their hands were professors from Tsinghua University and Renmin University of China, including Zhiyuan Liu, Jirong Wen, Jie Tang, Minlie Huang and others. Later, scholars from universities such as Peking University and the Chinese Academy of Sciences also expressed interest, and some members from outside BAAI also participated, such as Hongxia Yang who was working at the Alibaba DAMO Academy at the time. In the end, BAAI’s large-scale model project gathered about 100 people, and Jie Tang, who was then the vice president of BAAI, was appointed as the general director of the project.


Tang Jie

In October of that year, BAAI reported the "100-person large-scale model plan" to Chen Jining, the mayor of Beijing at the time. Mayor Chen said excitedly: "This (large-scale model) is the nuclear explosion point of artificial intelligence in the future. It will bring about the vigorous development of the entire production ecosystem.” The Beijing Municipality decided to strongly support and approve special funds for BAAI to purchase computing power.

In fact, at that time, many people still couldn’t clearly see the essence of large models, and the research and development of a large model required high funds. But in October 2020, from scholars to BAAI, from Beijing to the Ministry of Science and Technology, everyone reached a consensus, which is to fully promote the research and development of China's large-scale models. Afterwards, many scholars expressed their amazement to Leiphone: "It's strange that no one hesitated at that time."

Everyone thought that large models can be made into bigger things. In addition to large language models, the idea of "quantity causes qualitative change" may also produce breakthroughs in other fields. Therefore, after discussion, everyone decided to "divide the army into four groups" and develop China's large model from four directions: Chinese-language large-scale models, multi-modal large-scale model, cognitive large-scale models, and protein large-scale model – led by Liu Zhiyuan, Jirong Wen and Jie Tang in turn, of which Tang is responsible for the latter two, which is equivalent to "three teams doing four things."

In November 2020, the group discussed naming. At the Natural Language Processing Annual Conference held at the Chunhuiyuan resort in Shunyi District, Maosong Sun said that everyone is studying something language-related, and it is recommended to use the word "文" at the beginning. After the discussion, the four groups jointly named four of the seven imperial libraries that housed the "Siku Quanshu" in the Qing Dynasty, namely "Wenyuan", "Wenlan", "Wenhui" and "Wensu". .

In order to show that everyone is a whole, BAAI suggested a unified code name, and invited everyone to the Sai’er Building where BAAI was located in Wudaokou at that time. At the meeting, Tang Jie suggested that the name had something to do with Wudaokou, because everyone was in Wudaokou and had deep feelings for Wudaokou, so everyone thought of a few names together. After a brainstorm, Ruihua Song from Renmin University proposed to call it "Enlightenment" (wudao), which sounds like "五道" (wudao), and everyone agreed.

This is how "enlightenment" [悟道] name came about.
03 When the Wudao group’s stars shine

The original intention of Wudao is pure: to catch up with GPT-3 and research China’s large models.

So, what is “China’s large model"?

Nowadays, there are so many large models in China and so many types that the definition of large models is blurred. But in 2020, the awareness of Wudao members is very focused: in the final analysis, GPT-3 is a large-scale language model mainly in English, but China did not have it at that time. Therefore, “China’s large model” should first be a large-scale pre-trained language model with a parameter volume of 175 billion or more, mainly based in Chinese-language.

Although later studies have shown that language models based on a single language also have certain other capabilities in other languages, in the Chinese context, people found that using GPT-3 to solve many Chinese language tasks often leads to semantic differences and logical errors, and so on. One reason is that the training corpus of GPT-3 is mainly in English, and the other is that the Chinese research team has no way of knowing the detailed training parameters of GPT-3 to fine tune. Therefore, whether it was a subjective or objective decision, in 2020, self-developed domestic large-scale models were the undeniable choice.

BAAI was established in October 2020. Since large models require large computing power, BAAI also began to invest heavily in computing power and other resources from October. At the beginning, BAAI planned to use the existing scientific research funds to purchase 300P. Mayor Chen Jining made a decision to strongly support it, and then decided to allocate funds from the special funds to purchase 700P, so the total is 1000P. However, the process of going through approvals and purchasing computing power dragged on for more than a year, so Wudao mainly relied on renting computing power in its start-up phase.

Everyone believes that large-scale models are a major direction in the future, and relevant scholars also bring some of their own rations to participate in Zhiyuan's large-scale model projects: in terms of human-power, each professor brings masters and doctoral students from their teams to join; in terms of resources, when BAAI’s computing power was not fully in place, scholars also obtained part of the computing power through their own channels. For example, Jirong Wen's group initially trained multi-modal large models on Renmin University’s machines, and Tang Jie's group ran on the machine of Alibaba Cloud.

Although the thunder of GPT-3 was very loud, at that time, the Chinese team like Zhiyuan, which devoted itself to large-scale models, was not very beautiful, and Wudao was even bad-mouthed for a time. There are two main reasons for bad-mouthing: one is that the research and development of large models is very expensive, and the calculation cost can easily reach tens of millions; the second is that large models were not original innovations, and only relied on accumulating parameters, so the technical content was not high. However, BAAI was still "willing to go its own way" and was determined to explore.

After they really started researching, they discovered that OpenAI is not a swindler who is ostentatious, and the technical threshold of large models is not only "heaping computing power" and "heaping parameters". Taking Chinese large-scale models and multi-modal large-scale models as examples, before Wudao, the exploration of these two areas by the global AI community was blank. In addition, they are the first batch of people in China to train large models, which is equivalent to starting from scratch, and the process is very challenging.

But it is precisely because of this indomitable courage that half a year later, Wudao’s large models have made great progress.

Two months after the Wudao project was established, in December 2020, the Wenyuan team led by Zhiyuan Liu, Minlie Huang, and Wentao Han released the world's first Chinese open source model "CPM". The number of parameters of CPM is only 2.6 billion, which is insignificant compared with GPT-3, but the advantage is that it is based on a Chinese (data) corpus. In addition, compared with "ERNIE" in 2019, the parameter volume of CPM has been expanded several hundred times. This is not only a victory in engineering volume, but also verifies that the Wenyuan team's idea of training large Chinese models is feasible.

Wenlan and Wenhui also found a solution almost at the same time as CPM. The "Twin Towers" route of Lu Zhiwu, a core member of Wenlan Algorithm, was verified in December 2020, and Wenhui's large model with tens of billions of parameters was completed in January 2021. In March 2021, Zhiyuan will integrate Wenyuan's CPM, Wenlan's multimodal model BriVL 1.0 based on 30 million image-text data pairs training, and Wenhui's 10 billion parameter Chinese-English bilingual large model GLM-10B with the multimodal model CogView 1.0 and other achievements were brought together, collectively referred to as "Wudao 1.0" and released in March 2021.

Objectively speaking. "Wudao 1.0" did not cause much sensation, but at a time when China generally did not know about large models, Wudao told everyone "what is a large model", which can write poems, ask questions and answer questions, and align pictures with text... More powerful than all previous NLP algorithms.

***Excised two paragraphs about a press conference announcing Wudao 1.0

In the Wenhui group led by Tang Jie, Hongxia Yang, an engineer of Alibaba DAMO Academy, and Yang Zhilin, co-founder of Recurrent AI, are core members. BAAI did not restrict the research freedom of Wudao members. Yang participated in Alibaba’s large-scale model, and Yang Zhilin led the team to cooperate with Huawei. In April 2021, Alibaba also released the large-scale model "PLUG" with 27 billion parameters, and Huawei released Pangu. Wudao not only serves as a bridge between scholars, but also strengthens the cooperation between academia and industry.

***Excised paragraph about Wenhui team

But for China's large-scale models, the greater influence of high-performance computing is the birth of China's first trillion-scale model: "Enlightenment 2.0".

At the end of 2020, while promoting Wudao, Tang Jie, Chen Wenguang, and Hongxia Yang were also planning another thing, which is to apply for the Gordon Bell Prize known as the "Nobel Prize in the Field of Supercomputing Applications".

The supercomputer applying for the Gordon Bell Prize needs to meet several requirements: first, the supercomputer applied for must be the largest in the world; second, the research project on the machine must run to the full capacity of the machine; third, the results of the projects must be influential. After the completion of GLM-10B in January 2021, they decided to run the large model on the supercomputer.

So, they sent more than 30 people to run a large model on the "Sunway OceanLight" in the Qingdao laboratory. Among those who went in person, the students of Tang Jie and Zhai Jidong were the main force. Zhai Jidong was recruited by Tang Jie and Chen Wenguang because of his outstanding ability to do parallel training on low-level operators. In addition, there were several Alibaba engineers who provided support online.

They took all the data they collected to Qingdao, including Chinese-language, English-language, images, etc., and mixed them together for training. Because the requirement of the Gordon Bell Prize is to run the machine to full capacity, they expanded the number of parameters of the model to 174 trillion without any convergence on the data. After running on the supercomputing machine for ten days, they trained several versions of large models, each with a parameter volume of trillions.

Although the scale is large, the cost of operation is extremely high, which is beyond the affordability of almost everyone, so they trained a more convergent MoE-based model with a parameter volume of 1.75 trillion, which is ten times larger than GPT-3. It surpassed the 1.6 trillion parameter large-scale model Switch Transformer released by Google in April 2021, and became the largest large-scale model in the world at that time. It finally became the focus of the audience when it was released at the BAAI conference in June 2021, and was directly regarded as "Wudao". 2.0" has been widely praised by top technology teams at home and abroad.

BAAI achieved an unparalleled moment, and ranked among the top echelon of international large-scale models.

In addition to this trillion-dollar model, "Wudao 2.0" actually includes two tens of billions models of the Wenyuan team (Chinese-language model with 11 billion parameters, Chinese-English bilingual model with 11 billion parameters) and a 100-billion model (198 billion parameter Chinese-English bilingual MoE model), collectively referred to as "CPM 2.0"; the Wenlan team's 5 billion parameter graphic retrieval large model BriVL 2.0 - this is China's first multi-modal large model, and it is also the multi-modal large model with the largest number of parameters and the most training data in the world at that time.

***Excised two very technical paragraphs about the “Twin tower” method of building transformers

Afterwards, Lu Zhiwu and others told Leiphone that they did not think that they were "doing research by following behind other people's asses"; whether it is Chinese large-scale models, multi-modal large-scale models, or trillion-scale large-scale models, etc., Wudao’s three groups broke new ground by going deep into no-man's land.

In order to study multi-modal large models, Lu Zhiwu devoted all his students to Wenlan, and the team did not publish any academic papers for a whole year. In colleges and universities, this is a great risk for both professors and students.

Similarly, when studying large Chinese models, due to the lack of high-quality Chinese data, many students of Zhiyuan Liu and Minlie Huang were sent to do data labeling and cleaning. In the research of CPM 2.0, the original data collected by the Wenyuan team was as high as 50TB, and 2.6TB after cleaning. Students invested a lot of time and energy in it.

In general, BAAI and Wudao's 100 people are fighting with their backs, "gambling their own careers", but everyone did not expect that they won the bet: after the release of "Wudao 2.0" in June 2021, BAAI’s Wudao has become a distinctive flagbearer of China's large-scale models, and the members of Wudao have become the first batch of pioneers in China's large-scale models.
Second Half of the translation next week!
04 At the Eve of ChatGPT